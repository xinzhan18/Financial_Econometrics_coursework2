---
title: |
  | Financial Econometric in R/Python 
  | Assignment Two
author: |
  | Group 2
  | Hessa Alabbas 02513615
  | Xin Zhan 02299544
  | Alex Rached 01894052
  | Yan Cai 02381303
  | Kexin Liu 02362049
  |
  | The Business School, Imperial College London
  | 
date: "18-11-2023"
output:
  pdf_document: 
    keep_tex: true
    latex_engine: xelatex

header-includes:
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{caption}
---


\newpage
\tableofcontents
\listoftables
\newpage

\pagebreak

# Introduction

## Load Packages 
```{r loadLibs, message=FALSE, warning=FALSE}
library(lubridate)
library(knitr)
library(dplyr)
library(lmtest)
library(readxl)
library(moments)
library(sandwich)
library(estimatr)
library(margins) 
library(randomForest)
library(e1071) 
library(MASS) 
library(class) 
library(lmtest)
library(tree)
library(knitr)
library(rpart)
```


## Load Data
```{r}
data <-  read_excel('employment_08_09.xlsx')
```

# Question A
What fraction of workers in the sample were employed in April 2009? Use your answer to compute a 95% confidence interval for the probability that a worker was employed in April 2009, conditional on being employed in April 2008. 

```{r}

#sample size
n <- nrow(data)

#employed fraction
p <- sum(data$employed == 1) / n
cat("Fraction of workers employed in April 2009:", p, "\n")

#margin of error
margin <- qnorm(0.975)*sqrt(p*(1-p)/n)

#lower and upper intervals
lowerinterval <- p - margin
upperinterval <- p + margin

cat(lowerinterval,upperinterval)


```
The fraction of workers in the sample is  0.8754619. The interpretation is that, based on the sample data and statistical analysis, we are 95% confident that the true probability of a worker being employed in April 2009, given they were employed in April 2008, lies between 0.8666648 and 0.884259.


# Question B

Regress Employed on Age and Age**2 , using a linear probability model.

```{r}
binary_lm <- lm(employed ~ age + I(age ^ 2), data = data)
summary(binary_lm)
coeftest(binary_lm, vcov = vcovHC(binary_lm), type = "HC1")
```

## i)
Based on this regression, was the age a statistically significant determinant of employment in April 2009. 
\vspace{1em}


The positive coefficient for the 'Age' variable suggests that, on average, the probability of being employed increases with age. The coefficient is statistically significant with a p-value < 0.001, so there is a statistically significant relationship between age and employment in April 2009. Although it is statistically significant, the overall fit of the model (Multiple R-squared and Adjusted R-squared) indicates that age explains a very small proportion of the variability in employment status. In this case, only about 1.966% of the variability in employment status is explained by age and its squared term. The low p-value (< 2.2e-16) indicates that at least one of the predictors (age or age^2) is related to the dependent variable.


## ii)
Is there evidence of a nonlinear effect of age on probability of being employed? 
\vspace{1em}

Yes, there is evidence of a nonlinear effect of age on the probability of being employed based on the regression results. The negative coefficient for the squared term 'Age^2' is also statistically significant (p-value < 0.001). This suggests that as age increases, the positive effect of age on the probability of being employed diminishes, indicating a curvature or nonlinear pattern in the relationship.


## iii)
Compute the predicted probability of employment for a 20-year-old worker, a 40year-old worker, and a 60-year-old worker. 

```{r}
predicted_probabilities <- predict(binary_lm, 
                                   newdata = data.frame(age = c(20,40,60)), 
                                   type = "response")

print(predicted_probabilities)

```
The predicted probability of employment for a 20-year-old worker is approximately 74.23%.
The predicted probability of employment for a 40-year-old worker is approximately 91.58%.
The predicted probability of employment for a 60-year-old worker is approximately 82.79%.


# Question C
Repeat (b) using a probit regression.

```{r}
binary_probit <- glm(employed ~ age + I(age ^ 2), 
                     family = binomial(link = "probit"), 
                     data)
summary(binary_probit)
coeftest(binary_probit, vcov = vcovHC(binary_probit), type = "HC1")


```

The positive coefficient for 'age' (0.1217230) indicates that, on average, the log-odds of being employed increase with age. The negative coefficient for the squared term 'I(age^2)' (-0.0014125) suggests a nonlinear effect. As 'age' increases, the positive effect on the log-odds of being employed diminishes. The z-tests show that all coefficients are statistically significant at the 0.05 significance level, indicating that age and its squared term are significantly related to the log-odds of being employed.


## i) 
Based on this regression, was the age a statistically significant determinant of employment in April 2009. 
\vspace{1em}

Yes, based on the results of the probit regression model, age appears to be a statistically significant determinant of employment in April 2009. The positive coefficient for the 'age' variable suggests that, on average, the log-odds of being employed increase with age. This coefficient is statistically significant with a very low p-value (< 0.001), indicating strong evidence that the effect of age on employment is different from zero.

The low p-values indicate that both 'age' and 'age^2' are highly unlikely to have coefficients equal to zero, suggesting that both linear and nonlinear components of age are important in determining employment status.

## ii) 
Is there evidence of a nonlinear effect of age on probability of being employed?
\vspace{1em}

The negative coefficient for the squared term 'I(age^2)' (-0.0014) suggests a nonlinear effect. Specifically, as 'age' increases, the positive effect on the log-odds of being employed diminishes. This coefficient is also statistically significant with a very low p-value (< 0.001).

## iii) 
Compute the predicted probability of employment for a 20-year-old worker, a 40-year-old worker, and a 60-year-old worker.

```{r}

predicted_probabilities_probit <- predict(binary_probit,
                                          newdata = data.frame(age = c(20,40,60)),
                                          type = "response")

print(predicted_probabilities_probit)


```
The predicted probability of employment for a 20-year-old worker is approximately 72.96%.
The predicted probability of employment for a 40-year-old worker is approximately 91.17%.
The predicted probability of employment for a 60-year-old worker is approximately 83.16%.



# Question D
Repeat (b) using a logit regression.
```{r}
binary_logit <- glm(employed ~ age + I(age ^ 2), 
                    family = binomial(link = "logit"), 
                    data)
summary(binary_logit)
coeftest(binary_logit, vcov = vcovHC(binary_logit), type = "HC1")                     

```

## i) 
Based on this regression, was the age a statistically significant determinant of employment in April 2009. 
\vspace{1em}

Yes, age was a statistically significant determinant of employment in April 2009, as evidenced by the low p-values for both the 'Age' and 'I(age^2)' coefficients. The positive coefficient for 'Age' indicates a positive linear relationship. These findings highlight the importance of considering age as a factor influencing employment outcomes during the specified period. The residual deviance is 3972.9 on 5409 degrees of freedom, indicating a reasonable fit of the model to the data.

## ii) 
Is there evidence of a nonlinear effect of age on probability of being employed?
\vspace{1em}

Yes, there is evidence of a nonlinear effect of age on the probability of being employed, as indicated by the coefficient for the quadratic term 'I(age^2)' in the logistic regression model.

The coefficient for 'I(age^2)' is estimated to be -0.0026, and its associated p-value is < 2e-16, which is highly statistically significant. This implies that the relationship between age and the log-odds of employment is not purely linear but involves a quadratic component. In other words, the impact of age on employment probability is not constant; it changes nonlinearly with age. This finding underscores the importance of considering not only the linear effect of age but also its quadratic effect when modeling employment outcomes.

## iii)
Compute the predicted probability of employment for a 20-year-old worker, a 40-year-old worker, and a 60-year-old worker.

```{r}
predicted_probabilities_logit <- predict(binary_logit, 
                                         newdata = data.frame(age = c(20,40,60)), 
                                         type = "response")
print(predicted_probabilities_logit)
```
The predicted probability of employment for a 20-year-old worker is approximately 72.51%.
The predicted probability of employment for a 40-year-old worker is approximately 91.14%.
The predicted probability of employment for a 60-year-old worker is approximately 83.10%.


# Question E

Are there important differences in your answers to (b)-(d)? Explain.
\vspace{1em}


The estimated coefficients and standard errors of each independent variables (age and age^2) and associated intercepts are highest in logit regression and lowest in linear probability model. In addition, the estimated coefficient of intercept in linear probability model is positive, while negative for logit and probit model.

The predicted probability for employed for a 20-year-old and 40-year-old worker is highest in linear model, followed by logit and probit. The predicted probability for employed for a 60-year-old worker is highest in logit model, followed by linear and probit.

Coefficients and standard errors and predicted probabilities differ among models because of the different functional forms, including linear, logit and probit in our case.

# Question F

```{r}
# we_state and educ_adv are deleted for collinearity and 
# na number in earnwke are dropped as well
binary_lpm_modified <- lm(employed ~ age + I(age^2)+as.factor(race)+earnwke+
                            married+ne_states+so_states+ce_states+we_states+
                            educ_lths+ educ_hs+educ_somecol+educ_aa+educ_bac+
                            educ_adv+female, 
                        data,na.action = "na.omit")
summary(binary_lpm_modified)
coeftest(binary_lpm_modified, vcov = vcovHC(binary_lpm_modified), type = "HC1")
```

## (i) 

By adding those covariates to the linear probability model regression of point (b), investigate whether the conclusions on the effect of Age on employment from (b) are affected by omitted variable bias.
\vspace{1em}

The impact of age on employment is susceptible to omitted variable bias, a phenomenon characterized by two essential conditions: the independent variables must be correlated with the omitted variable, and the omitted variable must be a determinant of the dependent variable.

In our analysis, we try to address omitted variable bias by incorporating variables representing workers' educational attainment, gender, race, marital status, region of residence, and weekly earnings. From the regression, we find the coefficients for 'earnwke,' 'ce_states,' 'educ_lths,' and ' as.factor(race)2,' are all found to be statistically significant at the 1% level.

However, to discuss potential omitted variables, we focus on following ones.

The variable 'earnwke,' which captures average weekly earnings, satisfies both conditions for omitted variable bias. Firstly, average weekly earnings exhibit correlation with age, given that the elderly population typically earns less than their younger counterparts, owing to factors such as technological changes and physical limitations. Secondly, 'earnwke' serves as a determinant of employment, as individuals with lower salaries are more prone to job displacement. Consequently, the coefficients of age may experience downward bias due to the positive correlation between employment and 'earnwke,' and the negative correlation between 'earnwke' and age.

Similarly, the variable 'educ_lths,' indicating whether a worker's highest level of education is less than a high school graduate, satisfies both conditions. Firstly, education levels correlate with age, as the elderly tend to have lower educational exposure than younger groups. Secondly, education levels influence salary to some extent, reflecting increasing market demands for skilled workers. Consequently, the coefficients of age may suffer from downward bias due to the negative correlation between employment and 'educ_lths,' and the positive correlation between 'educ_lths' and age.

Moreover, the observed decrease in the magnitude (ignoring sign) of the coefficients for age and age^2 further validates the presence of downward bias resulting from omitted variables.


## (ii)
Use the regression results to discuss the characteristics of workers who were hurt the most by the 2008 financial crisis.
\vspace{1em}

From the regression results, I can conclude that those workers who are aged, black, having lower weekly earnings, not living in the central state and highest level of education is less than a high school graduate were hurt most by the 2008 financial crisis.



# Question G
Use the models in (b)-(d) to assess the in-sample accuracy of the classification. What is the proportion of correctly assigned classes?


```{r}
# Create a function to calucate the precision and recall
normalize_cm <- function(cm, type='precision') {
  if (type == 'precision') {
    col_sum <- colSums(cm)
    col_sum[col_sum == 0] <- 1
    precision_matrix <- sweep(cm, 2, col_sum, FUN="/")
    return(precision_matrix)
  } else if (type == 'recall') {
    row_sum <- rowSums(cm)
    row_sum[row_sum == 0] <- 1
    recall_matrix <- sweep(cm, 1, row_sum, FUN="/")
    return(recall_matrix)
  } else {
    stop("Type must be either 'precision' or 'recall'")
  }
}

# To get recall and precision from the matrix
extract_values <- function(matrix_recall,matrix_precision) {
  recall <- diag(matrix_recall)
  precision <- diag(matrix_precision)
  

  recall[is.na(recall)] <- 0
  precision[is.na(precision)] <- 0

  return(cbind(recall, precision))
}

```

```{r}
# LPM predictions
LPM_predictions_raw <- predict(binary_lm, newdata = data, type = "response")
LPM_predictions<- ifelse(LPM_predictions_raw > 0.5, 1, 0)

# Logit predictions
logit_predictions_raw <- predict(binary_logit, newdata = data, type = "response")
logit_predictions<- ifelse(logit_predictions_raw > 0.5, 1, 0)

# Probit predictions
probit_predictions_raw <- predict(binary_probit, newdata = data, type = "response")
probit_predictions<-  ifelse(probit_predictions_raw > 0.5, 1, 0)

LPM_accuracy    <- sum(LPM_predictions== data$employed) / nrow(data)
logit_accuracy  <- sum( logit_predictions== data$employed) / nrow(data)
probit_accuracy <- sum( probit_predictions== data$employed) / nrow(data)

all_accuracies <- c(logit_accuracy, probit_accuracy, LPM_accuracy)
names(all_accuracies) <- c("Logit", "Probit", "LPM")
kable(all_accuracies, caption = "Accuracy for Three Models")
```
The accuracies of three models are same.  

```{r}
confusion_matrix_LPM    <- table(Actual = data$employed, 
                                 Predicted = factor(LPM_predictions,
                                                    levels = c(0, 1)))
confusion_matrix_logit  <- table(Actual = data$employed, 
                                 Predicted = factor(logit_predictions,
                                                    levels = c(0,1)))
confusion_matrix_probit <- table(Actual = data$employed, 
                                 Predicted = factor(probit_predictions,
                                                    levels=c(0,1)))
confusion_matrix_LPM
```


These three models have identical confusion matrices, and from the results, we can observe the following:

True Negatives (TN): 0 (the number of actual class 0 predicted as class 0). The models did not correctly predict any instances that were actually class 0.

False Positives (FP): 674 (the number of actual class 0 predicted as class 1). All instances that were actually class 0 were incorrectly predicted as class 1.

False Negatives (FN): 0 (the number of actual class 1 predicted as class 0). The models did not incorrectly predict any instances that were actually class 1 as class 0.

True Positives (TP): 4738 (the number of actual class 1 predicted as class 1). The models correctly predicted all instances that were actually class 1.

This performance of the models may indicate a significant issue with data imbalance, or a bias in feature recognition and learning within the models, leading them to recognize only one class.



```{r}
confusion_matrix_LPM_precision <- normalize_cm(confusion_matrix_LPM)
confusion_matrix_logit_precision <- normalize_cm(confusion_matrix_logit)
confusion_matrix_probit_precision <- normalize_cm(confusion_matrix_probit)

confusion_matrix_LPM_recall <- normalize_cm(confusion_matrix_LPM,'recall')
confusion_matrix_logit_recall <- normalize_cm(confusion_matrix_logit,'recall')
confusion_matrix_probit_recall <- normalize_cm(confusion_matrix_probit,'recall')

values_LPM <- extract_values(confusion_matrix_LPM_recall,
                             confusion_matrix_LPM_precision)
values_Logit <- extract_values(confusion_matrix_logit_recall,
                               confusion_matrix_logit_precision)
values_Probit <- extract_values(confusion_matrix_probit_recall,
                                confusion_matrix_probit_precision)

performance_table <- data.frame(
  Model = rep(c("LPM", "Logit", "Probit"), each = 2),
  Class = rep(c("Class 0", "Class 1"), 3),
  
  Recall = c(values_LPM[, "recall"], 
             values_Logit[, "recall"], 
             values_Probit[, "recall"]),
  
  Precision = c(values_LPM[, "precision"],
                values_Logit[, "precision"], 
                values_Probit[, "precision"])
)


kable(performance_table, caption = "Recall and Precision for Three Models")

```


The table shows that the LPM, Logit, and Probit models have identical Recall and Precision values.

For Class 0:

Recall is 0, indicating that none of the actual Class 0 samples were correctly predicted as Class 0 by the models. This suggests poor performance of the models in identifying Class 0.
Precision is 0, meaning that none of the samples predicted as Class 0 were actually Class 0.

For Class 1:

Recall is 1, meaning all the samples that are actually Class 1 were correctly predicted as Class 1 by the models. This indicates good performance of the models in identifying Class 1.
Precision is approximately 0.8754619, indicating that about 87.55% of the samples predicted as Class 1 are actually Class 1.


Overall,

These three models perform well in predicting Class 1 (Class 1), exhibiting high Recall and fairly high Precision.
However, their performance in predicting Class 0 (Class 0) is poor, with both Recall and Precision being 0. This might suggest a tendency of these models to predict most or all samples as Class 1, overlooking Class 0.
This phenomenon is likely due to data imbalance (87.5% of the samples are Class 1), where Class 1 samples might significantly outnumber those of Class 0, leading to a bias in the models towards Class 1 during training.
The models might be overfitting to the features of Class 1, leading to a neglect of Class 0 in predictions.

To improve model performance, measures such as resampling to balance the data, adjusting model complexity to reduce overfitting, or introducing more distinctive features might be necessary. 



# Question H

Optional: Repeat point (g) using one or more (at your discretion) of the following classification algorithms: Naïve Bayes Classifier, Linear Discriminant Analysis, Quadratic Discriminant Analysis, Decision trees, Random forests, K-Nearest Neighbours.

## Build Models


```{r}
# Naïve Bayes
data$age_square <- data$age **2
nb_model <- naiveBayes(as.factor(employed) ~ age + age_square, data)
nb_predictions_raw <- predict(nb_model, newdata = data, type = "raw")
nb_predictions <- ifelse(nb_predictions_raw[,2] > 0.5, 1, 0)
nb_accuracy <- sum(nb_predictions == data$employed) / nrow(data)

confusion_matrix_nb <- table(Actual = data$employed, 
                             Predicted = factor(nb_predictions,levels = c(0, 1)))
confusion_matrix_nb_precision <- normalize_cm(confusion_matrix_nb)
confusion_matrix_nb_recall <- normalize_cm(confusion_matrix_nb,'recall')

cat(paste('model accuracy is',nb_accuracy))
print(confusion_matrix_nb)
```
The accuracy of Navie Bayes is 87.54%. The confusion matrix is same as before.



Linear Discriminant Analysis

```{r}
# Fit LDA Classifer 
lda_model <- lda(as.factor(employed) ~ age + I(age ^ 2), data)
lda_predictions <- predict(lda_model, newdata = data)$class
lda_accuracy <- sum(lda_predictions == data$employed) / nrow(data)

confusion_matrix_lda  <- table(Actual = data$employed, 
                               Predicted = factor(lda_predictions,levels = c(0, 1)))
confusion_matrix_lda_precision <- normalize_cm(confusion_matrix_lda)
confusion_matrix_lda_recall <- normalize_cm(confusion_matrix_lda,'recall')
print(lda_accuracy)
print(confusion_matrix_lda)
```
 
The accuracy of Linear Discriminant Analysis is 87.54%. The confusion matrix is same as before.
 
 
 
Quadratic Discriminant Analysis

```{r}
# Fit QDA Classifier
qda_model <- qda(as.factor(employed) ~ age + I(age ^ 2), data)
qda_predictions <- predict(qda_model, newdata = data)$class
qda_accuracy  <- sum(qda_predictions == data$employed) / nrow(data)

confusion_matrix_qda <- table(Actual = data$employed, 
                              Predicted = factor(qda_predictions,levels = c(0, 1)))
confusion_matrix_qda_precision <- normalize_cm(confusion_matrix_qda)
confusion_matrix_qda_recall <- normalize_cm(confusion_matrix_qda,'recall')

print(qda_accuracy)
print(confusion_matrix_qda)

```
The accuracy of Quadratic Discriminant Analysis is 86.10%. 



Decision trees

```{r}
# Decision Tree
set.seed(123) # Setting a seed for reproducibility

decision_tree_model <- rpart(as.factor(employed) ~ age +  I(age ^ 2), data = data)
dt_predictions <- predict(decision_tree_model, newdata = data, type = "class")

# Accuracy
dt_accuracy <- sum(dt_predictions == data$employed) /  nrow(data)

confusion_matrix_tree <- table(Actual = data$employed, 
                               Predicted = factor(dt_predictions,levels = c(0, 1)))
confusion_matrix_tree_precision <- normalize_cm(confusion_matrix_tree)
confusion_matrix_tree_recall <- normalize_cm(confusion_matrix_tree,'recall')
print(dt_accuracy)
print(confusion_matrix_tree)
```


The accuracy of Decision Tree is 87.54%. The confusion matrix is same as before.




Random forests


```{r}

# Random forests
rf_model <- randomForest(as.factor(employed) ~ age +  I(age ^ 2),
                         data= data, 
                         num.trees= 100) 
rf_predictions <- predict(rf_model , data)

# Accuracy
rf_accuracy <- mean(rf_predictions == data$employed)

confusion_matrix_rf <- table(Actual = data$employed, 
                             Predicted = factor(rf_predictions,levels = c(0, 1)))
confusion_matrix_rf_precision <- normalize_cm(confusion_matrix_rf)
confusion_matrix_rf_recall <- normalize_cm(confusion_matrix_rf,'recall')
print(rf_accuracy)
print(confusion_matrix_rf)

```


The accuracy of Random forests is 87.54%. The confusion matrix is same as before.



K-Nearest Neighbours


```{r}
# kNN Classifier
data$age_squared <- data$age ** 2 
scaled_data <- scale(data[, c("age", "age_squared")])
# Define the number of neighbors
k <- 7
knn_predictions <- knn(train = scaled_data, test = scaled_data, 
                       cl = as.factor(data$employed), k = k)
knn_accuracy  <- sum(knn_predictions == data$employed) / nrow(data)

confusion_matrix_knn <- table(Actual = data$employed, 
                              Predicted = factor(knn_predictions,levels = c(0, 1)))
confusion_matrix_knn_precision <- normalize_cm(confusion_matrix_knn)
confusion_matrix_knn_recall <- normalize_cm(confusion_matrix_knn,'recall')
print(knn_accuracy)
print(confusion_matrix_knn)
```

The accuracy of K-Nearest Neighbours is 87.54%. The confusion matrix is same as before.


## Horizontal Comparison of Accuracy

```{r}

comparison_table <- data.frame(
  Model = c("naiveBayes", 
            " LDA Classifer", 
            "QDA Classifier",
            "Decision Tree",
            "Random Forest",
            "KNN"),
  
  Accuracy = c(nb_accuracy, 
               lda_accuracy, 
               qda_accuracy,
               dt_accuracy,
               rf_accuracy,
               knn_accuracy)
)

kable(comparison_table, caption = "Accuracy for All Models")

```

This table showcases a comparison of accuracy rates for different machine learning models. Accuracy, a measure of a classification model's performance, represents the proportion of correct predictions (both positive and negative classes). 

For models such as NaiveBayes, LDA Classifier, Decision Tree, Random Forest, and KNN, the accuracy stands at 0.8754619 (approximately 87.55%). High accuracy rates for these models suggest good performance on the given task.

For the QDA Classifier, the accuracy is slightly lower, at 0.8610495 (about 86.10%), marginally below the other models.

Comprehensive Analysis:

The accuracy rates of these models are very close, indicating minimal performance differences between them on this specific task. The slightly lower accuracy of the QDA Classifier might be attributed to the model's characteristics or the features of the dataset.
High accuracy rates demonstrate the models' overall precise predictions, but this doesn't necessarily mean that the models perform equally well across all types of predictions. For instance, a model might predict more accurately in one category than another.
To better evaluate the results, a comparison of their recall and precision is necessary.



## Horizontal Comparison of Recall and Precision


```{r}


values_nb <- extract_values(confusion_matrix_nb_recall,confusion_matrix_nb_precision)
values_lda <- extract_values(confusion_matrix_lda_recall,confusion_matrix_lda_precision)
values_qda <- extract_values(confusion_matrix_qda_recall,confusion_matrix_qda_precision)
values_tree <- extract_values(confusion_matrix_tree_recall,confusion_matrix_tree_precision)
values_rf <- extract_values(confusion_matrix_rf_recall,confusion_matrix_rf_precision)
values_knn <- extract_values(confusion_matrix_knn_recall,confusion_matrix_knn_precision)


performance_table <- data.frame(
  Model = rep(c("NaiveBayes", 
                "LDA", 
                "QDA",
                "DecisiomTree",
                "RandomForest",
                "KNN"), each = 2),
  
  Class = rep(c("Class 0", "Class 1"), 3),
  
  Recall = c(values_nb[, "recall"],
             values_lda[, "recall"],
             values_qda[, "recall"],
             values_tree[,"recall"],
             values_rf[,"recall"],
             values_knn[,"recall"]),
  
  Precision = c(values_nb[, "precision"], 
                values_lda[, "precision"],
                values_qda[, "precision"],
                values_tree[,"precision"],
                values_rf[,"precision"],
                values_knn[,"precision"])
)



kable(performance_table, caption = "Recall and Precision for All Models")


```

This table presents the Recall and Precision of multiple models for two categories (Class 0 and Class 1). For the majority of the models (naiveBayes, LDA, Decision Tree, RandomForest, KNN):


Class 0:

Both Recall and Precision are 0. This indicates that these models failed to correctly predict any sample that actually belongs to Class 0, showing poor performance.

Class 1:

Recall is 1, and Precision is 0.8754619. This means these models successfully predicted all samples that actually belong to Class 1, but of all the samples predicted as Class 1, approximately 87.55% are indeed Class 1. These models excel in predicting Class 1 but completely fail to predict Class 0.


For QDA:

Class 0:

Recall is 0.0816024, and Precision is 0.2925532. This suggests that QDA has some capability in predicting Class 0, albeit limited.

Class 1:

Recall is 0.9719291, and Precision is 0.8815084. This indicates that QDA is highly effective and relatively precise in predicting Class 1. QDA shows a more balanced performance in predicting both categories compared to other models.


Summary:

Most models demonstrate a significant preference for Class 1, effectively predicting Class 1 but completely failing in predicting Class 0. This performance may be related to data imbalance, where Class 1 samples possibly far outnumber Class 0.
QDA shows a more balanced predictive capability for both categories, though it's relatively weaker in Class 0 but still significantly better than other models.

Despite the high accuracy of these models (about 87.55%), primarily driven by their strong performance in Class 1, high accuracy does not necessarily mean good predictive ability across all categories, especially in cases of data imbalance.
Relying solely on accuracy can be misleading when dealing with imbalanced data. Hence, Recall and Precision become important complementary metrics.

To enhance model performance in Class 0, specific strategies might be required, such as resampling, using different evaluation metrics (like the F1 score), or trying different models and feature engineering methods.


## Improvement

```{r}
#install.packages("ROSE")
#install.packages("DMwR")
library(ROSE)

clean_data <- na.omit(data)


clean_data <- clean_data %>% 
  mutate(employed=as.factor(employed)) %>% 
  mutate(race=as.factor(race))

clean_data <- subset(clean_data, select = -unemployed)

set.seed(123)  # 设置种子以确保结果可重复
sample_size <- floor(0.70 * nrow(clean_data))  # 计算70%的数据量
train_index <- sample(seq_len(nrow(clean_data)), size = sample_size)
train_data <- clean_data[train_index, ]
test_data <- clean_data[-train_index, ]

majority <- train_data[train_data$employed == 1, ]
minority <- train_data[train_data$employed == 0, ]

balanced_data <- ovun.sample(employed~.,data = train_data,
                            p=0.5,seed=1,method = "over")$data

# 检查平衡后的类别分布
table(balanced_data$employed)

rf_model <- randomForest(as.factor(employed) ~. , data= balanced_data, num.trees= 100) 

rf_predictions <- predict(rf_model , balanced_data)
rf_predictions_test <- predict(rf_model , test_data)

# Accuracy

rf_accuracy <- sum(rf_predictions == balanced_data$employed)/ nrow(balanced_data)
rf_accuracy_test <- sum(rf_predictions_test == test_data$employed)/ nrow(test_data)

rf_accuracy
rf_accuracy_test


confusion_matrix_rf <- table(Actual = factor(balanced_data$employed,levels=c(0,1)), 
                             Predicted = factor(rf_predictions,levels = c(0, 1)))

confusion_matrix_rf_precision <- normalize_cm(confusion_matrix_rf)
confusion_matrix_rf_recall <- normalize_cm(confusion_matrix_rf,'recall')

confusion_matrix_rf

confusion_matrix_rf_test <- table(Actual = test_data$employed, 
                             Predicted = factor(rf_predictions_test,levels = c(0, 1)))
confusion_matrix_rf_test_precision <- normalize_cm(confusion_matrix_rf_test)
confusion_matrix_rf_test_recall <- normalize_cm(confusion_matrix_rf_test,'recall')

confusion_matrix_rf_test

importance(rf_model)
```